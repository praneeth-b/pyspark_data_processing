{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd97b9de-408f-45f1-937f-69d9e733e161",
   "metadata": {},
   "source": [
    "#                                   Yelp ETL Pipeline \n",
    "\n",
    "- **Medallion Architecture**: The data pipeline for the Yelp data is implemented following the Medallion Architecture, which includes the following 3 stages:\n",
    "  - **Bronze**: Raw ingestion (store data as-is, minimal transformations).\n",
    "  - **Silver**: Cleaned and enriched data.\n",
    "  - **Gold**: Aggregated, business-ready data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369e10c7-376a-408d-bd7d-a275baea6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all dependencies\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql.functions import current_timestamp, lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f1d624-8a15-41df-af42-cacf59c4c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/28 14:30:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# creat Spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YelpDataPipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"64\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", 200 * 1024 * 1024) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1058dda-915a-46aa-a903-cbff8af9c39c",
   "metadata": {},
   "source": [
    "## 1. Bronze Layer\n",
    "The following cell demonstrates the **Bronze layer** of a Medallion architecture ETL pipeline for the Yelp dataset where Yelp JSON files are ingested and stored as Parquet files.\n",
    "\n",
    "The class `RawDataLoader` encapsulates the logic for:\n",
    "- Reading Yelp JSON files\n",
    "- Adding ingestion metadata: Source and timestamp.\n",
    "- Writing them as Parquet into the Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e23b719-c8cd-492e-8189-690d5b4fe415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataLoader:\n",
    "    \"\"\"\n",
    "    The Raw data from the json files defined in the config file are loaded as parquet files into the bronze layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession, raw_path: str, bronze_path: str):\n",
    "        \n",
    "        self.spark = spark\n",
    "        self.raw_path = raw_path\n",
    "        self.bronze_path = bronze_path\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_json_to_bronze(self, dataset_name: str):\n",
    "        \"\"\"Load JSON and save as Parquet in bronze layer\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            json_file = os.path.join(self.raw_path, f\"yelp_academic_dataset_{dataset_name}.json\")\n",
    "            bronze_table = os.path.join(self.bronze_path, dataset_name)\n",
    "\n",
    "            self.logger.info(f\"Loading {dataset_name} from {json_file}\")\n",
    "\n",
    "            # Read Json file\n",
    "            df = self.spark.read.json(json_file)\n",
    "\n",
    "            # Add metadata columns to track the ingested timestamp and source file.\n",
    "            df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                .withColumn(\"source_file\", lit(json_file))\n",
    "\n",
    "            # Write to bronze as Parquet format. No data modification/filter/clean performed at this step.\n",
    "            df.write.mode(\"overwrite\").parquet(bronze_table)\n",
    "\n",
    "            row_count = df.count()\n",
    "            self.logger.info(f\"Loaded {row_count} rows to bronze/{dataset_name}\")\n",
    "            return row_count\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load {dataset_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_all_datasets(self, datasets: list):\n",
    "        \"\"\"Load all the files specified in config file. The datasets(json files) of the Yelp dataset are defined in\n",
    "           the config.yaml file\n",
    "\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for dataset in datasets:\n",
    "            results[dataset] = self.load_json_to_bronze(dataset)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4d5171-34cb-480e-ad33-60499aa1174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 14:30:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer load results: {'business': 150346, 'review': 6990280, 'user': 1987897, 'checkin': 131930, 'tip': 908915}\n"
     ]
    }
   ],
   "source": [
    "# Define paths \n",
    "raw_path = \"data/raw\"   # path of the raw json files \n",
    "bronze_path = \"data/bronze\" # destination path of the ingested bronze data \n",
    "\n",
    "loader = RawDataLoader(spark, raw_path, bronze_path)\n",
    "\n",
    "# Example: Load all datasets\n",
    "datasets = [\"business\", \"review\", \"user\", \"checkin\", \"tip\"]\n",
    "results = loader.load_all_datasets(datasets)\n",
    "print(\"Bronze layer load results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cf89d8-7804-43a4-bd98-b6cc5c8f2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+----------+-----------+------------------+-----------+------------+-----+-----+--------------------+--------------------+\n",
      "|             address|          attributes|         business_id|          categories|        city|               hours|is_open|  latitude|  longitude|              name|postal_code|review_count|stars|state| ingestion_timestamp|         source_file|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+----------+-----------+------------------+-----------+------------+-----+-----+--------------------+--------------------+\n",
      "|  5223 S Macdill Ave|{NULL, NULL, NULL...|LS49tIPKSRgWekZOP...|Tobacco Shops, Sh...|       Tampa|{22:0-20:0, 22:0-...|      0|27.8885924|-82.4942119|Ta Francisco Cigar|      33611|           6|  4.5|   FL|2025-11-28 14:30:...|data/raw/yelp_aca...|\n",
      "|4516 S Dale Mabry...|{NULL, NULL, u'fu...|kp9XiB8PbjLrDyzed...|Nightlife, Pubs, ...|       Tampa|{11:0-23:0, 11:0-...|      0|27.9014971|-82.5059198|  Beef 'O' Brady's|      33611|          29|  2.5|   FL|2025-11-28 14:30:...|data/raw/yelp_aca...|\n",
      "| 6800 Rising Sun Ave|{NULL, NULL, 'non...|WHzeDa97M_eU1FEZa...|Food, Coffee & Te...|Philadelphia|{0:0-0:0, 0:0-0:0...|      1| 40.055688| -75.090405|              Wawa|      19111|          18|  3.5|   PA|2025-11-28 14:30:...|data/raw/yelp_aca...|\n",
      "|  2588 Memorial Blvd|                NULL|Y3LoBZpCQQbB6ZgNO...|Nail Salons, Beau...| Springfield|                NULL|      1|  36.48855| -86.863901|          Nail Tek|      37172|           8|  2.0|   TN|2025-11-28 14:30:...|data/raw/yelp_aca...|\n",
      "|      1111 Walnut St|{NULL, NULL, u'fu...|fneeyCOYV5-oVeQP0...|American (Traditi...|Philadelphia|{11:30-20:0, 11:3...|      0| 39.948898| -75.159717|  Destination Dogs|      19107|         329|  4.5|   PA|2025-11-28 14:30:...|data/raw/yelp_aca...|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+--------------------+-------+----------+-----------+------------------+-----------+------------+-----+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bronze_business = spark.read.parquet(os.path.join(bronze_path, \"business\"))\n",
    "# bronze_business.printSchema()\n",
    "bronze_business.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621663e9-6ff5-45ec-b27a-eb9d85f02e6c",
   "metadata": {},
   "source": [
    "## 2. Silver Layer\n",
    "The silver layer logic mainly deals with cleaning and transforming the raw Bronze layer data.\n",
    "The main steps involved in the **silver layer** are:\n",
    "- Select essential columns\n",
    "- Standardize data types\n",
    "- Filter invalid/missing records\n",
    "- Deduplicate rows\n",
    "\n",
    "\n",
    "\n",
    "This class `DataCleaner` encapsulates cleaning logic for each Yelp dataset:\n",
    "- **Business**: Standardize data types, keep essential fields, remove nested attributes column, validate stars, deduplicate .\n",
    "                  Optimization: At the end of silver layer, the business_silver dataframe is cached since it is used multiple times in the gold layer.\n",
    "- **Review**: Standardize data types, standardize dates, filter invalid ratings, deduplicate.\n",
    "- **Checkin**: Standardize data types, explode comma‑separated dates, convert to timestamp/date, deduplicate.\n",
    "- **User**: Standardize data types, filter nulls, deduplicate.\n",
    "- **Tip**:  Standardize data types, filter null values.\n",
    "\n",
    "For more details check the docstrings of each class method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9520101-a499-4c2e-8f05-3e21b1bc54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, when, trim, regexp_replace, to_timestamp, to_date, explode, split\n",
    "import logging\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def clean_business(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Clean the business dataset (original source: yelp_academic_dataset_business.json)       \n",
    "            - Keep essential columns for downstream analytics and standardize data types\n",
    "            - Remove overly nested/complex structures that need separate processing : Here Attributes column is highly \n",
    "              nested and does not have a consistent structure. Hence removing it out.\n",
    "            - Check for Null values and valid range of stars column and deduplicate.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning business data\")\n",
    "        # selecting required columns and casting them to a suitable data type.\n",
    "        column_selected_df = df.select(\n",
    "            trim(col(\"business_id\")).alias(\"business_id\").cast(\"string\"),\n",
    "            trim(col(\"name\")).alias(\"business_name\").cast(\"string\"),\n",
    "            col(\"address\").cast(\"string\"),\n",
    "            col(\"city\").cast(\"string\"),\n",
    "            col(\"state\").cast(\"string\"),\n",
    "            col(\"postal_code\").cast(\"string\"),\n",
    "            col(\"latitude\").cast(\"double\"),\n",
    "            col(\"longitude\").cast(\"double\"),\n",
    "            col(\"stars\").cast(\"double\"),\n",
    "            col(\"review_count\").cast(\"int\"),\n",
    "            col(\"is_open\").cast(\"int\"),\n",
    "            col(\"categories\")\n",
    "        )\n",
    "\n",
    "        filtered_df = column_selected_df.filter(\n",
    "            # Remove records with missing critical fields\n",
    "            col(\"business_id\").isNotNull() &\n",
    "            col(\"business_name\").isNotNull() &\n",
    "            col(\"stars\").isNotNull() &\n",
    "            (col(\"stars\").between(0, 5))  # Valid star range\n",
    "        )\n",
    "\n",
    "        depuplicated_df = filtered_df.dropDuplicates([\"business_id\"])\n",
    "\n",
    "\n",
    "        return depuplicated_df\n",
    "\n",
    "    def clean_review(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" Clean review dataset source: yelp_academic_dataset_review.json\n",
    "            - select required columns and trim required columns for leading/lagging spaces and standardize data types.\n",
    "            - Check for nulls and valid range for respective columns\n",
    "            - Remove duplicate reviews\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Cleaning review data\")\n",
    "\n",
    "        column_selection_df =  df.select(\n",
    "            trim(col(\"review_id\")).alias(\"review_id\").cast(\"string\"),\n",
    "            trim(col(\"user_id\")).alias(\"user_id\").cast(\"string\"),\n",
    "            trim(col(\"business_id\")).alias(\"business_id\").cast(\"string\"),\n",
    "            col(\"stars\").cast(\"double\"),\n",
    "            col(\"useful\").cast(\"int\"),\n",
    "            col(\"funny\").cast(\"int\"),\n",
    "            col(\"cool\").cast(\"int\"),\n",
    "            to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\")).alias(\"review_date\"), # Convert to timestamp when it is string\n",
    "            col(\"text\").cast(\"string\").alias(\"review_text\")\n",
    "        )\n",
    "\n",
    "\n",
    "        filtered_df = column_selection_df.filter(\n",
    "            col(\"review_id\").isNotNull() &\n",
    "            col(\"user_id\").isNotNull() &\n",
    "            col(\"business_id\").isNotNull() &\n",
    "            col(\"stars\").isNotNull() &\n",
    "            (col(\"stars\").between(0, 5))\n",
    "        )\n",
    "\n",
    "        deduplicated_df = filtered_df.dropDuplicates([\"review_id\"])\n",
    "\n",
    "        return deduplicated_df\n",
    "\n",
    "    def clean_checkin(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Clean checkin dataset  source: yelp_academic_dataset_checkin.json\n",
    "            - Explode comma seperated checkin_time column to multiple rows and standardize data types.\n",
    "            - Enrich with checkin_timestamp and checkin_date columns.\n",
    "            - Check for Null values and deduplicate checkin data\n",
    "            \n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning checkin data\")\n",
    "\n",
    "\n",
    "        # Explode comma-separated dates\n",
    "        checkin_explode_df = df.select(\n",
    "            col(\"business_id\"),\n",
    "            explode(split(col(\"date\"), \", \")).alias(\"checkin_time\")\n",
    "        )\n",
    "\n",
    "        checkin_column_format_df = (\n",
    "            checkin_explode_df\n",
    "        .withColumn(\n",
    "            \"checkin_timestamp\", to_timestamp(col(\"checkin_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        .withColumn(\n",
    "            \"checkin_date\", to_date(col(\"checkin_timestamp\")))\n",
    "        )\n",
    "\n",
    "        checkin_selected_df = checkin_column_format_df.select(\n",
    "            trim(col(\"business_id\")).alias(\"business_id\").cast(\"string\"),\n",
    "            col(\"checkin_timestamp\"),\n",
    "            col(\"checkin_date\")\n",
    "        )\n",
    "\n",
    "        checkin_filtered_df = checkin_selected_df.filter(\n",
    "            col(\"business_id\").isNotNull() &\n",
    "            col(\"checkin_timestamp\").isNotNull()\n",
    "        )\n",
    "\n",
    "        checkin_deduplicated_df = checkin_filtered_df.dropDuplicates([\"business_id\", \"checkin_timestamp\"])\n",
    "\n",
    "        return checkin_deduplicated_df\n",
    "\n",
    "    def clean_user(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Clean user dataset  source: yelp_academic_dataset_user.json\n",
    "            - Trim values and standardize data types\n",
    "            - Select required columns\n",
    "            - Check for nulls and deduplicate\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning user data\")\n",
    "\n",
    "        user_selected_df = df.select(\n",
    "            trim(col(\"user_id\")).alias(\"user_id\").cast(\"string\"),\n",
    "            trim(col(\"name\")).cast(\"string\").alias(\"user_name\"),\n",
    "            col(\"review_count\").cast(\"int\"),\n",
    "            to_date(to_timestamp(col(\"yelping_since\"), \"yyyy-MM-dd HH:mm:ss\")).alias(\"yelping_since\"),\n",
    "            col(\"useful\").cast(\"int\"),\n",
    "            col(\"funny\").cast(\"int\"),\n",
    "            col(\"cool\").cast(\"int\"),\n",
    "            col(\"fans\").cast(\"int\"),\n",
    "            col(\"average_stars\").cast(\"double\")\n",
    "        )\n",
    "\n",
    "        user_filtered_df = user_selected_df.filter(     # to add more filters\n",
    "            col(\"user_id\").isNotNull()\n",
    "        )\n",
    "\n",
    "        user_deduplicated_df = user_filtered_df.dropDuplicates([\"user_id\"])\n",
    "\n",
    "        return user_deduplicated_df\n",
    "\n",
    "    def clean_tip(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Clean tip dataset  source: yelp_academic_dataset_tip.json\n",
    "            - Trim and standardize data types\n",
    "            - Select required columns\n",
    "            - Check for null values\n",
    "            \n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning tip data\")\n",
    "\n",
    "        tip_selected_df = df.select(\n",
    "            trim(col(\"user_id\")).alias(\"user_id\").cast(\"string\"),\n",
    "            trim(col(\"business_id\")).alias(\"business_id\").cast(\"string\"),\n",
    "            col(\"text\").cast(\"string\").alias(\"tip_text\"),\n",
    "            to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\")).alias(\"tip_date\"),\n",
    "            col(\"compliment_count\").cast(\"int\")\n",
    "        )\n",
    "\n",
    "        tip_filtered_df = tip_selected_df.filter(\n",
    "            col(\"user_id\").isNotNull() &\n",
    "            col(\"business_id\").isNotNull()&\n",
    "            col(\"tip_text\").isNotNull()\n",
    "        )\n",
    "\n",
    "        # avoiding deduplicate check since same user_id can tip the same business_id multiple times. There is no tip_id column.\n",
    "\n",
    "        return tip_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d342aa11-5239-46fd-8955-0b04730e32fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 150346\n",
      "+----------------------+---------------------------+---------------------------+--------------+-----+-----------+----------+-----------+-----+------------+-------+------------------------------------------------------------------+\n",
      "|business_id           |business_name              |address                    |city          |state|postal_code|latitude  |longitude  |stars|review_count|is_open|categories                                                        |\n",
      "+----------------------+---------------------------+---------------------------+--------------+-----+-----------+----------+-----------+-----+------------+-------+------------------------------------------------------------------+\n",
      "|--30_8IhuyMHbSOcNWd6DQ|Action Karate              |2235 York Rd               |Jamison       |PA   |18929      |40.2553619|-75.0883992|3.5  |9           |1      |Trainers, Active Life, Fitness & Instruction, Karate, Martial Arts|\n",
      "|--OS_I7dnABrXvRCCuWOGQ|Lens Auto Body & Painting  |4819 Cottman Ave           |Philadelphia  |PA   |19135      |40.0281603|-75.0338026|4.0  |5           |1      |Auto Repair, Car Rental, Automotive, Hotels & Travel, Body Shops  |\n",
      "|--pDYWb4DzqKdAdrPcxuaA|First Flight Wine Bar      |Tampa International Airport|Tampa         |FL   |33604      |28.0142184|-82.4163459|4.0  |13          |1      |Wine Bars, Nightlife, Bars                                        |\n",
      "|-0fvhILrC9UsQ6gLNpZlTQ|David's Southern Fried Pies|8601 Frankford Ave         |Philadelphia  |PA   |19136      |40.046191 |-75.0150899|4.5  |18          |0      |Desserts, Food                                                    |\n",
      "|-0jK77zdE3-plqXuwXtilQ|Salerno's III              |1292 Lower Ferry Rd        |Ewing Township|NJ   |08628      |40.2717529|-74.7908719|4.0  |68          |1      |Italian, Seafood, Restaurants, Pizza, Sandwiches                  |\n",
      "+----------------------+---------------------------+---------------------------+--------------+-----+-----------+----------+-----------+-----+------------+-------+------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating the cleaning step for bronze data\n",
    "bronze_business = spark.read.parquet(\"data/bronze/business\")\n",
    "bronze_checkin = spark.read.parquet(\"data/bronze/checkin\")\n",
    "bronze_review = spark.read.parquet(\"data/bronze/review\")\n",
    "bronze_tip = spark.read.parquet(\"data/bronze/tip\")\n",
    "bronze_user = spark.read.parquet(\"data/bronze/user\")\n",
    "\n",
    "cleaner = DataCleaner(spark)\n",
    "silver_business = cleaner.clean_business(bronze_business)\n",
    "silver_business.write.mode(\"overwrite\").parquet(\"data/silver/business\")\n",
    "\n",
    "silver_checkin = cleaner.clean_checkin(bronze_checkin)\n",
    "silver_checkin.write.mode(\"overwrite\").parquet(\"data/silver/checkin\")\n",
    "\n",
    "silver_review = cleaner.clean_review(bronze_review)\n",
    "silver_review.write.mode(\"overwrite\").parquet(\"data/silver/review\")\n",
    "\n",
    "silver_tip = cleaner.clean_tip(bronze_tip)\n",
    "silver_tip.write.mode(\"overwrite\").parquet(\"data/silver/tip\")\n",
    "\n",
    "silver_user = cleaner.clean_user(bronze_user)\n",
    "silver_user.write.mode(\"overwrite\").parquet(\"data/silver/user\")\n",
    "\n",
    "# printing silver business data\n",
    "print(f\"Rows after cleaning: {silver_business.count()}\")\n",
    "silver_business.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee707d-5d29-4914-bedb-767c3ec6ba33",
   "metadata": {},
   "source": [
    "## 3. Gold Layer\n",
    "\n",
    "The **Gold Layer** deals with the aggregation of the Silver Layer data in order to derive meaningful business insights. The following aggregations are performed:\n",
    "- Weekly star ratings per business\n",
    "- Check‑ins in a business compared to overall star rating.\n",
    "\n",
    "The class `DataAggregator` encapsulates aggregation logic for the Gold layer:\n",
    "\n",
    "- Method `aggregate_weekly_stars(reviews_df, business_df)` : This method receives the 'reviews' and 'business' data from the silver layer. The aggregation logic is:\n",
    "    -  The reviews data is enriched with the columns review_week and review_week_number to extract the week information from the review_date column.\n",
    "    -  The reviews data is grouped by (\"business_id\", \"review_year\", \"review_week_number\", \"review_week\") and weekly aggregate star values and other rating values are derived.\n",
    "    -  The business data  is now left-joined with  weekly aggregated reviews data in order to combine business related information to the weekly aggregated stars.\n",
    "    - Optimization:\n",
    "      - The spark.sql.autoBroadcastJoinThreshold is set to 200 MB so that smaller dataframes like the business dataframe are broadcast during joins.\n",
    "      - The weekly_stars aggregated table is partitioned on 'review_week' so that future querying becomes efficient.\n",
    "\n",
    "        \n",
    "- Method `aggregate_checkins_vs_stars(checkin_df, business_df)` : This method receives the 'checkin' and 'business' data from the silver layer. The aggregation logic is:\n",
    "    - Total checkins per business is derived by grouping the checkins data by \"business_id\" and counting the number of checkins.\n",
    "    - The business dataframe containing business information such as overall star rating is left joined with the aggregated checkins dataframe in order to derive the Check‑ins in a\n",
    "      business compared to overall star rating.\n",
    "    - Additional attributes such as 'checkins_per_review' and 'star_category' is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275bb4a5-b9d0-49db-91eb-75e16de6bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, date_trunc, coalesce, lit,\n",
    "    weekofyear, year, round as spark_round, when, sum as spark_sum\n",
    ")\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class DataAggregator:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def aggregate_weekly_stars(self, reviews_df: DataFrame, business_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate stars per business on a weekly basis.\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Aggregating weekly stars per business\")\n",
    "\n",
    "        # derive weekly aggregations on review data.\n",
    "        weekly_agg = reviews_df.withColumn(\n",
    "            \"review_week\", date_trunc(\"week\", col(\"review_date\"))\n",
    "        ).withColumn(\n",
    "            \"review_year\", year(col(\"review_date\"))\n",
    "        ).withColumn(\n",
    "            \"review_week_number\", weekofyear(col(\"review_date\"))\n",
    "        ).groupBy(\n",
    "            \"business_id\", \"review_year\", \"review_week_number\", \"review_week\"   # review_week is redundant but might be useful for edge cases.\n",
    "        ).agg(\n",
    "            spark_round(avg(\"stars\"), 2).alias(\"avg_stars_weekly\"),\n",
    "            count(\"review_id\").alias(\"review_count_weekly\"),\n",
    "            spark_round(avg(\"useful\"), 2).alias(\"avg_useful\"),\n",
    "            spark_round(avg(\"funny\"), 2).alias(\"avg_funny\"),\n",
    "            spark_round(avg(\"cool\"), 2).alias(\"avg_cool\")\n",
    "        )\n",
    "        print(f\"rows of review agg: {weekly_agg.count()}\")\n",
    "\n",
    "        # define defaults to use in coalesce()\n",
    "        default_week_number = 0\n",
    "        default_year = 9999\n",
    "        default_review_week = lit('9999-01-01 00:00:00').cast(TimestampType())\n",
    "        # Join with business info\n",
    "        result = (business_df.join(weekly_agg, \"business_id\", how= \"left\"\n",
    "                                   ).select(\n",
    "            \"business_id\",\n",
    "            \"business_name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            col(\"stars\").alias(\"overall_stars\"),\n",
    "            coalesce('review_week', default_review_week).alias('review_week'),\n",
    "            coalesce('review_year', lit(default_year)).alias('review_year'),\n",
    "            coalesce('review_week_number', lit(default_week_number)).alias('review_week_number'),\n",
    "            coalesce('avg_stars_weekly', lit(0)).alias('avg_stars_weekly'),\n",
    "            coalesce('review_count_weekly', lit(0)).alias('review_count_weekly'),\n",
    "            coalesce('avg_useful', lit(0)).alias('avg_useful'),\n",
    "            coalesce('avg_funny', lit(0)).alias('avg_funny'),\n",
    "            coalesce('avg_cool', lit(0)).alias('avg_cool')\n",
    "        ).orderBy(\"business_id\", \"review_year\", \"review_week_number\"))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def aggregate_checkins_vs_stars(\n",
    "            self, checkin_df: DataFrame, business_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate checkins per business compared to overall star rating\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Aggregating check-ins vs star ratings\")\n",
    "\n",
    "        # Count check-ins per business\n",
    "        checkin_counts = checkin_df.groupBy(\"business_id\").agg(\n",
    "            count(\"checkin_timestamp\").alias(\"total_checkins\")\n",
    "        )\n",
    "\n",
    "        # Join with business ratings\n",
    "        result = business_df.join(\n",
    "            checkin_counts,\n",
    "            \"business_id\",\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"business_name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"stars\",\n",
    "            \"review_count\",\n",
    "            when(col(\"total_checkins\").isNull(), 0)\n",
    "            .otherwise(col(\"total_checkins\")).alias(\"total_checkins\")\n",
    "        ).withColumn(\n",
    "            \"checkins_per_review\",\n",
    "            spark_round(col(\"total_checkins\") / col(\"review_count\"), 2)\n",
    "        ).withColumn(\n",
    "            \"star_category\",      # adding an additional attribute based on star category\n",
    "            when(col(\"stars\") >= 4.0, \"High\")\n",
    "            .when(col(\"stars\") >= 2.5, \"Medium\")\n",
    "            .otherwise(\"Low\")\n",
    "        )\n",
    "\n",
    "        return result.orderBy(col(\"total_checkins\").desc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ecb59d-c1a5-46d6-81c2-5e7e61d6ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows of silver_reviews:6990280. Rows of business: 150346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows of review agg: 5138585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of weekly_stars: 5138585\n",
      "Weekly Stars Aggregation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------------------------------+---------------+-----+-------------+-------------------+-----------+------------------+----------------+-------------------+----------+---------+--------+\n",
      "|business_id           |business_name                                  |city           |state|overall_stars|review_week        |review_year|review_week_number|avg_stars_weekly|review_count_weekly|avg_useful|avg_funny|avg_cool|\n",
      "+----------------------+-----------------------------------------------+---------------+-----+-------------+-------------------+-----------+------------------+----------------+-------------------+----------+---------+--------+\n",
      "|---kPU91CF4Lq2-WlRu9Lw|Frankie's Raw Bar                              |New Port Richey|FL   |4.5          |2021-11-22 00:00:00|2021       |47                |4.67            |3                  |0.0       |0.0      |0.0     |\n",
      "|--0iUa4sNDFiZFrAdIWhZQ|Pupuseria Y Restaurant Melba                   |Clementon      |NJ   |3.0          |2013-06-24 00:00:00|2013       |26                |3.5             |2                  |2.0       |0.5      |0.0     |\n",
      "|--7jw19RH9JKXgFohspgQw|McIlwain Family Dentistry & Ahrens Orthodontics|Wesley Chapel  |FL   |4.0          |2015-06-08 00:00:00|2015       |24                |5.0             |3                  |1.0       |0.0      |0.0     |\n",
      "|--9osgUCSDUWUkoTLdvYhQ|Will You Escape?                               |Tucson         |AZ   |5.0          |2015-09-07 00:00:00|2015       |37                |5.0             |2                  |1.5       |0.0      |1.0     |\n",
      "|--ARBQr1WMsTWiwOKOj-FQ|Traveling Corks                                |Tampa          |FL   |4.5          |2016-01-11 00:00:00|2016       |2                 |5.0             |2                  |0.0       |0.0      |0.0     |\n",
      "+----------------------+-----------------------------------------------+---------------+-----+-------------+-------------------+-----------+------------------+----------------+-------------------+----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Rows of silver_checkin: 13353332\n",
      "Rows of checkin_vs_stars aggregated: 150346\n",
      "Check-ins vs Stars Aggregation:\n",
      "+----------------------+------------------------------------------------------+------------+-----+-----+------------+--------------+-------------------+-------------+\n",
      "|business_id           |business_name                                         |city        |state|stars|review_count|total_checkins|checkins_per_review|star_category|\n",
      "+----------------------+------------------------------------------------------+------------+-----+-----+------------+--------------+-------------------+-------------+\n",
      "|-QI8Qi8XWH3D8y8ethnajA|Philadelphia International Airport - PHL              |Philadelphia|PA   |2.5  |2149        |52129         |24.26              |Medium       |\n",
      "|FEXhWNCMkv22qG04E83Qjg|Café Du Monde                                         |New Orleans |LA   |4.0  |1880        |40092         |21.33              |High         |\n",
      "|Eb1XmmLWyt_way5NNZ7-Pw|Louis Armstrong New Orleans International Airport  MSY|Kenner      |LA   |3.0  |1789        |37553         |20.99              |Medium       |\n",
      "|c_4c5rJECZSfNgFj7frwHQ|Tampa International Airport                           |Tampa       |FL   |4.0  |1849        |37511         |20.29              |High         |\n",
      "|4i4kmYm9wgSNyF1b6gKphg|Nashville International Airport - BNA                 |Nashville   |TN   |3.5  |949         |31163         |32.84              |Medium       |\n",
      "+----------------------+------------------------------------------------------+------------+-----+-----+------------+--------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Load Silver datasets\n",
    "silver_reviews = spark.read.parquet(\"data/silver/review\")\n",
    "silver_business = spark.read.parquet(\"data/silver/business\")\n",
    "silver_checkin = spark.read.parquet(\"data/silver/checkin\")\n",
    "\n",
    "aggregator = DataAggregator(spark)\n",
    "\n",
    "# Weekly stars aggregation\n",
    "print(f\"rows of silver_reviews:{silver_reviews.count()}. Rows of business: {silver_business.count()}\")\n",
    "weekly_stars = aggregator.aggregate_weekly_stars(silver_reviews, silver_business)\n",
    "weekly_stars.write.partitionBy(\"review_week\").mode(\"overwrite\").parquet(f\"data/gold/weekly_stars\")\n",
    "print(f\"Rows of weekly_stars: {weekly_stars.count()}\")\n",
    "print(\"Weekly Stars Aggregation:\")\n",
    "weekly_stars.filter(col('review_count_weekly')>1).show(5, truncate=False)\n",
    "\n",
    "# Check-ins vs stars aggregation\n",
    "print(f\"Rows of silver_checkin: {silver_checkin.count()}\")\n",
    "checkins_vs_stars = aggregator.aggregate_checkins_vs_stars(silver_checkin, silver_business)\n",
    "checkins_vs_stars.write.mode(\"overwrite\").parquet(\"data/gold/checkins_vs_stars\")\n",
    "print(f\"Rows of checkin_vs_stars aggregated: {checkins_vs_stars.count()}\")\n",
    "print(\"Check-ins vs Stars Aggregation:\")\n",
    "checkins_vs_stars.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51693d34-9410-460a-ad84-7e4b64096b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152cb23d-e5d4-4f9a-9ea5-33f0826ea5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------------------------+-------------+-----+--------------+-------------+\n",
      "|business_id           |business_name                             |city         |state|avg_stars_2020|total_reviews|\n",
      "+----------------------+------------------------------------------+-------------+-----+--------------+-------------+\n",
      "|DboqYyH-S8pV6WxaF9Plow|Cal Coast Adventures                      |Santa Barbara|CA   |5.0           |72           |\n",
      "|gqOmu_puGr6VY0IGRLHtSA|Michael Richardson - Bay Equity Home Loans|Reno         |NV   |5.0           |54           |\n",
      "|S6AP1meHAC0RxJXW26fq1g|Philly Foodworks                          |Philadelphia |PA   |5.0           |52           |\n",
      "|aAXo_vX8YG10kytAM9SC9Q|Ice Dreammm Shop                          |Lutz         |FL   |5.0           |51           |\n",
      "|U6gikR4uhRl4zU8q3j02oA|The Artist Haus                           |Philadelphia |PA   |5.0           |50           |\n",
      "|oy1al7IJ75ZH_Fh9ho8zwg|Flow Pros                                 |St Petersburg|FL   |5.0           |50           |\n",
      "|zsk5qJ179aZvgtz62JCj7A|Santa Barbara Bikes To Go                 |Santa Barbara|CA   |5.0           |36           |\n",
      "|B_Kk8Nq9NqfvwFrN8uOR9w|City & Suburbs Pest Control Company       |Philadelphia |PA   |5.0           |33           |\n",
      "|oNGQMJmwwPWD1ELyAV9Rjg|Sugar Bar Salon                           |Philadelphia |PA   |5.0           |32           |\n",
      "|gXzd2Ao9NNLqCVUOcevdYQ|Chanta European Cuisine                   |Tampa        |FL   |5.0           |31           |\n",
      "+----------------------+------------------------------------------+-------------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample analytical query to find the top business in a given year.\n",
    "\n",
    "top_businesses = (\n",
    "    weekly_stars.filter(col(\"review_year\") == 2020)\n",
    "           .groupBy(\"business_id\", \"business_name\", \"city\", \"state\")\n",
    "           .agg(\n",
    "               spark_round(avg(\"avg_stars_weekly\"), 2).alias(\"avg_stars_2020\"),\n",
    "               spark_sum(\"review_count_weekly\").alias(\"total_reviews\")\n",
    "           )\n",
    "           .orderBy(col(\"avg_stars_2020\").desc(), col(\"total_reviews\").desc())\n",
    "           .limit(10)\n",
    ")\n",
    "\n",
    "top_businesses.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
